## English Words

| english                | 中文     |
| ---------------------- | -------- |
| directional derivative | 方向导数 |
| batch                  | 批       |
| tune                   | 调整     |
| invariance             | 不变性   |
| subsume                | 包含     |
| arbitrary              | 任意的   |



## 回顾

### 浅编码

Encoder是向量检索表，一个矩阵包含了所有节点的嵌入向量

### Deep graph encoder

用多层非线性转换

## 深度学习

把任务最终转换成优化问题：
$$
\min_{\Theta} L(y, f(x))
$$
$\Theta$即需要优化的参数，再浅编码中，$\Theta$即整个嵌入矩阵

L是损失函数，有L2 loss, L1 loss, huber loss,  max margin, cross entropy等

### 梯度下降

<img src="..\pics\gradient_descent.png" alt="image-20220116201509259" style="zoom:50%;" />

$\eta$为学习率

每次梯度下降都要计算一次$f(x)$中的$x$，计算量大，因此有随机梯度下降

### 随机梯度下降

每一步中，从总体中提取一个minibatch，作为input$x$

### 前向传播

从前到后计算损失的过程

### 反向传播

从损失从后到前链式法则求梯度的过程

### 非线性函数

是指$f(x)$中的函数，若全为线性，多层变换实质上是矩阵连乘，等价于一个矩阵

有ReLU和Sigmoid

## 图深度学习

用矩阵表示图，再用矩阵深度学习的办法行不通，从置换节点顺序后，结果变化就可以看出

### 图神经网络模型设计

1. 定义邻居节点聚集函数，要求permutation invariaty，可以用mean, sum, max
2. 定义损失函数
3. 在一个节点集合中进行训练
4. 生成节点的嵌入向量



