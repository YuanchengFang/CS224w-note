#### Computing Node Embeddings

We not only put nodes into the mini-batch, but also their **computational graphs**.

![image-20220725210802623](..\pics\Stochastic_training_of_GNN.png)

两个问题：

1. 计算图的大小随层数的增加，呈指数增长
2. 当遇到hub node (度特别大的节点)，计算图的大小会爆炸

#### Neighbor Sampling

Key idea: Construct the computational graph by (randomly) **sampling at most H neighbors** at each hop

1. 采样个数H的大小是在效率和稳定性之间tradeoff
2. 计算图的大小还是随层数指数增长，增加一层，计算大小就会倍增K
3. 怎么采样邻居？随机采当然快但不是最好，因为会采样到一些不重要的节点。更好的做法是用Random Walk with Restarts，采样score最高的H个邻居

Neighbor sampling相关的话题也是值得研究的课题

