#### GNN设计问题

一个任务上表现很好的GNN可能在另一个上就不行，为每一个新任务重新网格搜索调参不太可行。

#### A GNN design space

- Intra-layer design

  BN, dropout, activation, aggregation

- Inter-layer design

  Layer Connectivity (Stack, Skip-gram, Skip-cat...), pre-process layers, message passing layers, post-process layers

- Learning configuration

  Batch size, learning rate, optimizer, training epochs

We cannot cover all the possible designs, but propose a mindset transition (思维方式的转变): 学习一个设计空间比只学习特定GNN设计更有效。

#### A general GNN task space

The taxonomy of GNN tasks like (node / edge / graph level) is not precise enough, for example, predict CC of a node vs predict a node's subject area are both node classification task but completely different. 

Innovation: 定量分析不同GNN task的相似度

......

Some findings:

- BN is better
- Dropout is not needed, because GNNs suffer more from underfitting rather than overfitting.
- PRelu is good activation function
- Sum is good aggregator, (most expressive aggregator)
- Skip connection is better.
- adam is generally better.
- training more epochs is generally better

......