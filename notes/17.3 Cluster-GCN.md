有时计算图的构建会多余，尤其是当mini-batch中的一些节点共享很多邻居时，这些共享节点的计算图实际上是一样的，为mini-batch中每个节点构造计算图的过程中就因此产生了很多多余的计算。

一个办法是识别出多余的计算图构建，例如一篇论文HAGs

另一种办法是Cluster-GCN

#### Recall: Full Batch GNN

![image-20220725215259479](..\pics\full-batch-gnn.png)

Layer-wise (逐层) node embedding update能够复用上一层的embeddings，并且减少了邻居采样所需的计算量。但不好在大图上进行

#### Subgraph sampling

sample a small subgraph of the large graph and then perform the efficient full-batch layer-wise training on the subgraph.

#### Cluster-GCN

##### "vanilla" Cluster-GCN

![image-20220725220028759](..\pics\Cluster_GCN_vanilla.png)

This method ignores **between-group links**, which could hurt the GNN's performance.

还有一点是这种集群中的节点不够多样化，使得根据这个集群计算出的梯度不可靠，可能会使训练结果在集群之间波折......

![image-20220725220705080](..\pics\Issue_with_Cluster-GCN.png)

##### Advanced Cluster-GCN

Aggregate multiple node groups per mini-batch

#### Time Complexity Comparison

![image-20220725221108272](..\pics\Comparison_of_time_complexity.png)

#### Summary

![image-20220725221630921](..\pics\summary_of_cluster-GCN.png)

Cluster-GCN会导致异常的梯度估计，因此neighbor sampling可能用的更多一些。