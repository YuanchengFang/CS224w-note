expressivity 表现力

stochastic 随机的

log-likelihood objective 对数似然估计目标

$\sigma()$ : sigmoid function $\sigma(x) = \frac{1}{1+e^{-x}}, \R \Rightarrow (0,1)$

##  Random Walk估计结点的相似度

u,v结点相似度$\approx$ u,v在同一次random walk中同时出现的概率$\approx z_u^Tz_v$

### 优点

- 表现力：如果从u出发的random walk以很大概率访问v，则认为u与v相似
- 高效率：训练时考虑局部结点，不用关注所有结点

### Random Walk 优化过程

1. 先用某种random walk策略对每个结点walk一次

2. 对于每个结点u，收集到访问过的邻居$N_R(u)$

3. 采取最大似然估计，优化嵌入向量$z_u$，最大化
   $$
   \max_{f} \sum_{u\in V} \log P({N_R(u)}|z_u)
   $$
   其中，从u出发walk经过v的概率预测为
   $$
   P(v|z_u) = \frac{\exp(z_u^Tz_v)}{\sum_{n\in V}{\exp(z_u^Tz_n)}}
   $$

总结，就是要最小化L:

<img src="..\pics\random_walk_optimization.png" alt="image-20220104211143815" style="zoom: 50%;" />

### 进一步性能优化：负采样

注意到$P(v|z_u)$分母要对所有结点进行计算，实际上，按度越大概率越大的方法取k个结点即可，同时用上sigmoid函数

<img src="..\pics\negative_sample.png" alt="image-20220104212857688" style="zoom:50%;" />

## Random Walk 中的策略

最简单的是定长无偏地随机游走

而注意到结点的访问共有两种方式：DFS和BFS

- BFS考察微观局部的邻居
- DFS考察宏观全局的邻居

可以通过定长但是对BFS和DFS两种方式设定比例来实现有偏袒的random walk，具体是两个系数p和q

- Return parameter: p指代返回到上一次访问的结点的可能性，这也意味着要实时记录上一次访问的结点
- In-out parameter: q指代访问更远的结点（不与上一次访问结点相邻）的可能性
- p越大，越倾向于BFS，q越大，越倾向于DFS

## node2vec

1. 计算随机游走概率（决定下一步去哪）
2. 对每个结点，进行定长的random walk
3. 用随机梯度下降Stochastic Gradient Descent优化目标向量

线性时间复杂度，且3个步骤可以并行

## 简要回顾

衡量结点相似度的方法：

- naive：相邻即相似
- 邻居重叠
- random walk

### 应用场景

- node2vec适合结点分类
- 其他适合链路预测